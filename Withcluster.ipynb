{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of k-means clustering algorithm.\n",
    "These functions are designed to work with cartesian data points\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "def convert_to_2d_array(points):\n",
    "    \"\"\"\n",
    "    Converts `points` to a 2-D numpy array.\n",
    "    \"\"\"\n",
    "    points = np.array(points)\n",
    "    if len(points.shape) == 1:\n",
    "        points = np.expand_dims(points, -1)\n",
    "    return points\n",
    "def visualize_clusters(clusters):\n",
    "    \"\"\"\n",
    "    Visualizes the first 2 dimensions of the data as a 2-D scatter plot.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    for cluster in clusters:\n",
    "        points = convert_to_2d_array(cluster)\n",
    "        if points.shape[1] < 2:\n",
    "            points = np.hstack([points, np.zeros_like(points)])\n",
    "        plt.plot(points[:,0], points[:,1], 'o')\n",
    "    plt.show()\n",
    "\n",
    "def DTW(points,centroid):\n",
    "    errors = np.zeros((len(points),))\n",
    "    for i in range(len(points)):\n",
    "        errors[i]=DTWDistance(points[i],centroid)\n",
    "    return errors\n",
    "\n",
    "def DTWDistance(s1,s2):\n",
    "        '''\n",
    "        Calculates dynamic time warping Euclidean distance between two\n",
    "        sequences. Option to enforce locality constraint for window w.\n",
    "        '''\n",
    "        DTW={}\n",
    "        w=None\n",
    "        if w:\n",
    "            w = max(w, abs(len(s1)-len(s2)))\n",
    "\n",
    "            for i in range(-1,len(s1)):\n",
    "                for j in range(-1,len(s2)):\n",
    "                    DTW[(i, j)] = float('inf')\n",
    "\n",
    "        else:\n",
    "            for i in range(len(s1)):\n",
    "                DTW[(i, -1)] = float('inf')\n",
    "            for i in range(len(s2)):\n",
    "                DTW[(-1, i)] = float('inf')\n",
    "\n",
    "        DTW[(-1, -1)] = 0\n",
    "\n",
    "        for i in range(len(s1)):\n",
    "            if w:\n",
    "                for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "                    dist= (s1[i]-s2[j])**2\n",
    "                    DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "            else:\n",
    "                for j in range(len(s2)):\n",
    "                    dist= (s1[i]-s2[j])**2\n",
    "                    DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "        #print(\"DWT\",np.sqrt(DTW[len(s1)-1, len(s2)-1]))\n",
    "        return np.sqrt(DTW[len(s1)-1, len(s2)-1])\n",
    "\n",
    "def SSE(points):\n",
    "    \"\"\"\n",
    "    Calculates the sum of squared errors for the given list of data points.\n",
    "    \"\"\"\n",
    "    points = convert_to_2d_array(points)\n",
    "    centroid = np.mean(points, 0)\n",
    "    #errors= DTWDistance(points,centroid)\n",
    "    errors = np.linalg.norm(points-centroid, ord=2, axis=1)\n",
    "    #print(\"errors\",type(errors))\n",
    "    return np.sum(errors)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kmeans(points, k=2, epochs=1, max_iter=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Clusters the list of points into `k` clusters using k-means clustering\n",
    "    algorithm.\n",
    "    \"\"\"\n",
    "    points = convert_to_2d_array(points)\n",
    "    #print(\"points_nit\",points)\n",
    "    assert len(points) >= k, \"Number of data points can't be less than k\"\n",
    "    best_sse = np.inf\n",
    "    for ep in range(epochs):\n",
    "        print(\"epochs\",ep)\n",
    "        # Randomly initialize k centroids\n",
    "        ##np.random.shuffle(points)\n",
    "        centroids = points[0:k, :]\n",
    "        last_sse = np.inf\n",
    "        for it in range(max_iter):\n",
    "            print(\"iter\",it)\n",
    "            # Cluster assignment\n",
    "            clusters = [None] * k\n",
    "            for p in points:\n",
    "                \n",
    "                index = np.argmin(DTW(centroids,p))\n",
    "               \n",
    "                \n",
    "                if clusters[index] is None:\n",
    "                    clusters[index] = np.expand_dims(p, 0)\n",
    "                else:\n",
    "                    clusters[index] = np.vstack((clusters[index], p))\n",
    "            # Centroid update\n",
    "            \n",
    "            centroids=[]\n",
    "            for c in clusters:\n",
    "                if c is not None:\n",
    "                    centroids.append(np.mean(c,0))\n",
    "            #centroids = [np.mean(c, 0) for c in clusters]\n",
    "            # SSE calculation\n",
    "            #sse = np.sum([SSE(c) for c in clusters])\n",
    "            ssel=[]\n",
    "            for c in clusters:\n",
    "                if c is not None:\n",
    "                    ssel.append(SSE(c))\n",
    "            sse = np.sum(ssel)\n",
    "            gain = last_sse - sse\n",
    "            if verbose:\n",
    "                print((f'Epoch: {ep:3d}, Iter: {it:4d}, '\n",
    "                       f'SSE: {sse:12.4f}, Gain: {gain:12.4f}'))\n",
    "            # Check for improvement\n",
    "            if sse < best_sse:\n",
    "                best_clusters, best_sse = clusters, sse\n",
    "        # Epoch termination condition\n",
    "            if np.isclose(gain, 0, atol=0.00001):\n",
    "                break\n",
    "            last_sse = sse\n",
    "    return best_clusters\n",
    "def bisecting_kmeans(points, k, epochs=10, max_iter=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Clusters the list of points into `k` clusters using bisecting k-means\n",
    "    clustering algorithm. Internally, it uses the standard k-means with k=2 in\n",
    "    each iteration.\n",
    "    \"\"\"\n",
    "    points = convert_to_2d_array(points)\n",
    "    clusters = [points]\n",
    "    #print(\"len::clusters\",clusters)\n",
    "    while len(clusters) < k:\n",
    "        \n",
    "        max_sse_i = np.argmax([SSE(c) for c in clusters])\n",
    "        #print(\"max_sse_i\",max_sse_i)\n",
    "        cluster = clusters.pop(max_sse_i)\n",
    "        print(\"len\",len(clusters))\n",
    "        two_clusters = kmeans(cluster, 2, epochs=epochs, max_iter=max_iter, verbose=verbose)\n",
    "        clusters.extend(two_clusters)\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import nan\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "i=0\n",
    "ccc=0\n",
    "def normalise(X):\n",
    "    for i in range(len(X[0])):\n",
    "        v = X[:,i]\n",
    "        X[:,i] = (v - v.min())/(v.max() - v.min())\n",
    "    return X\n",
    "\n",
    "user_data = []\n",
    "user_level_understanding = []\n",
    "for subdir, dirs, files in os.walk('DataSet_name'):\n",
    "    print(\"user\",i)\n",
    "    j=0\n",
    "    if i==0:\n",
    "        i+=1\n",
    "        continue\n",
    "    labels = {}\n",
    "    levels_und = {}\n",
    "    attentive_sec = 0\n",
    "    for file in files:\n",
    "        print(\"video \",j)\n",
    "        filepath = os.path.join(subdir, file)\n",
    "        X = np.genfromtxt(filepath, delimiter=',')#,skip_header=None)\n",
    "        \n",
    "        X = normalise(X)\n",
    "        #print(type(X))\n",
    "        #print(X)\n",
    "       \n",
    "        y=0 # 0- not understood, 1 - understood\n",
    "        #Silhouette coefficient to find optimal k value\n",
    "        range_n_clusters = [2, 3, 4, 5, 6]\n",
    "        optimalK = -1\n",
    "        maxS = -1\n",
    "        for n_clusters in range_n_clusters:\n",
    "\n",
    "            clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "            cluster_labels = clusterer.fit_predict(X)\n",
    "            silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "            print(\"For n_clusters =\", n_clusters,\n",
    "                  \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "            if(silhouette_avg>maxS):\n",
    "                maxS=silhouette_avg\n",
    "                optimalK=n_clusters\n",
    "\n",
    "        #cluster = ts_cluster()\n",
    "\n",
    "        #algorithm = kmeans\n",
    "        algorithm = bisecting_kmeans\n",
    "        k = optimalK\n",
    "        verbose = False\n",
    "        max_iter = 3\n",
    "        epochs = 1\n",
    "        clusters = algorithm(X, optimalK, verbose=verbose, max_iter=max_iter, epochs=epochs)\n",
    "        \n",
    "        \n",
    "        centroids=[]\n",
    "        for c in clusters:\n",
    "            if c is not None:\n",
    "                centroids.append(np.mean(c,0))\n",
    "        duration = len(X)      \n",
    "        print(\"Duration of the video:\",len(X),duration)\n",
    "        print(\"no.of clusters::\",len(clusters))\n",
    "        print(\"sizes of clusters:\")\n",
    "        total=0\n",
    "        attentive_sec=0\n",
    "        for cluster in clusters:\n",
    "            points = convert_to_2d_array(cluster)\n",
    "            \n",
    "            #print(\"nit\",len(points))\n",
    "            #print(\"nit\",type(points))\n",
    "            #print(points)\n",
    "            X = points\n",
    "            fs = 512  # Sampling rate (512 Hz)\n",
    "\n",
    "            # Get real amplitudes of FFT (only in postive frequencies)\n",
    "            fft_vals = np.absolute(np.fft.rfft(X))    #these are my fft values rfft returns only the part of the result that \n",
    "                                                  #corresponds to nonpositive frequences. (Avoids complex conjugaes) \n",
    "                                                  #faster and for plotting\n",
    "\n",
    "            # Get frequencies for amplitudes in Hz\n",
    "    \n",
    "            fft_freq = np.fft.rfftfreq(len(X), 1.0 / fs)     # that might be fixed (window length n , and  sample spacing)\n",
    "                                                        #inverse of the sampling rate   returns sample freq of length n .\n",
    "\n",
    "            # Define EEG bands\n",
    "            eeg_bands = {'Delta': (0, 4),\n",
    "                        'Theta': (4, 8),\n",
    "                        'Alpha': (8, 12),\n",
    "                        'Beta': (12, 30),\n",
    "                        'Gamma': (30, 45)}\n",
    "\n",
    "            # Take the mean of the fft amplitude for each EEG band\n",
    "            eeg_band_fft5 = dict()\n",
    "            for band in eeg_bands:\n",
    "                freq_ix = np.where((fft_freq >= eeg_bands[band][0]) &   #np.where is like asking \"tell me where in this array, entries satisfy a given condition\".\n",
    "                            (fft_freq <= eeg_bands[band][1]))[0]    #for fft_frreq at all point where it satisfies it returns the index (in array)\n",
    "                                                             #if fftfreq[np.where bla bla] will give values array\n",
    "                eeg_band_fft5[band] = np.mean(fft_vals[freq_ix])\n",
    "    \n",
    "           # Plot the data (using pandas here cause it's easy)\n",
    "            print(\".................ans.........................................\")\n",
    "            print(\"Beta\",eeg_band_fft5['Beta'])\n",
    "            print(\"Alpha\",eeg_band_fft5['Alpha'])\n",
    "            print(\"Theta\",eeg_band_fft5['Theta'])\n",
    "            \n",
    "            if np.isnan(eeg_band_fft5['Beta']):\n",
    "                eeg_band_fft5['Beta'] =1 \n",
    "            if np.isnan(eeg_band_fft5['Alpha']) :\n",
    "                eeg_band_fft5['Alpha'] =1 \n",
    "            print(\"Beta\",eeg_band_fft5['Beta'])\n",
    "            print(\"Alpha\",eeg_band_fft5['Alpha'])\n",
    "            print(\"Theta\",eeg_band_fft5['Theta'])    \n",
    "            print(\"....................ans......................................\")\n",
    "            print(\"File_name\",(eeg_band_fft5['Beta']/eeg_band_fft5['Alpha']))\n",
    "            print(\"Duration of the video:\",len(X))\n",
    "            print(\"..........................................................\")\n",
    "            attention_value= (eeg_band_fft5['Beta']/eeg_band_fft5['Alpha'])\n",
    "            if ((eeg_band_fft5['Beta']/eeg_band_fft5['Alpha'])>1): \n",
    "                attentive_sec+= len(points)\n",
    "                #total = total + attention_value * len(points)\n",
    "        #print(\"attentive_sec\",attentive_sec) \n",
    "        print(\"attentive_sec\",attentive_sec)\n",
    "               \n",
    "        if (attentive_sec >= (duration*(3/5))):\n",
    "            y=1\n",
    "        labels[j] = y\n",
    "        print(duration)\n",
    "        level_und = attentive_sec/duration\n",
    "        print(level_und)\n",
    "        levels_und[j] = level_und\n",
    "        visualize_clusters(clusters)     \n",
    "\n",
    "        j+=1\n",
    "    i=i+1\n",
    "    user_data.append(labels)\n",
    "    user_level_understanding.append(levels_und)\n",
    "\n",
    "u=1\n",
    "for user in user_data:\n",
    "    print(\"student \"+str(u))\n",
    "#    for k,v in user.items():\n",
    "#        if v == 0:\n",
    "#            user[k]=\"No\"\n",
    "#        else:\n",
    "#            user[k] = \"Yes\"\n",
    "    print(user)\n",
    "    u=u+1\n",
    "print(\"understanding level\")\n",
    "print(user_level_understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt('All_with_cluster_B_1sec.csv', with_cluster_B_1sec, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
